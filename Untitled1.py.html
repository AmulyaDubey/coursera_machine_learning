#!/usr/bin/env python
# coding: utf-8

# In[3]:


# used for manipulating directory paths
import os

# Scientific and vector computation for python
import numpy as np

# Plotting library
from matplotlib import pyplot
from mpl_toolkits.mplot3d import Axes3D  # needed to plot 3-D surfaces

# tells matplotlib to embed plots within the notebook
get_ipython().run_line_magic('matplotlib', 'inline')


# In[8]:


def warmUpExercise():
    
    A=np.identity(5,dtype=int) 
    return A


# In[9]:


warmUpExercise()


# In[15]:


data = np.loadtxt('/Users/apple/Desktop/ex1data1.txt', delimiter=',')
X, y = data[:, 0], data[:, 1]

m = y.size 


# In[18]:


print(data)


# In[35]:


def plotData(x,y):
 fig=pyplot.figure()
 pyplot.plot(x, y, 'ro', ms=10, mec='b')
 pyplot.ylabel('Profit in $10,000')
 pyplot.xlabel('Population of City in 10,000s')


# In[36]:


plotData(X,y)


# In[26]:


get_ipython().run_line_magic('pinfo', 'pyplot.plot')


# In[37]:


X = np.stack([np.ones(m), X], axis=1)


# In[38]:


print(X)


# In[50]:


def computeCost(X,y,theta): 
     J=np.dot(X,theta)-y
     J=np.dot(J,J)
     J=J/2
     J=J/m
     return J


# In[52]:


J=computeCost(X,y,theta=np.array([0.0,0.0]))
print('With theta = [0,0]\nCost computed=%.2f'%J)
print('Expected cost value 32.07\n')
J=computeCost(X,y,theta=np.array([-1,2]))
print('With theta = [-1,2]\nCost computed=%.2f'%J)
print('Expected cost value 52.24\n')


# In[53]:


def gradientDescent(X, y, theta, alpha, num_iters):
   
    m = y.shape[0]  
    theta = theta.copy()
    
    J_history = []
    
    for i in range(num_iters):
         theta=theta.copy()
         theta=theta.copy()-alpha*np.dot(X.T,np.dot(X,theta)-y)/m
         J_history.append(computeCost(X, y, theta))
    
    return theta, J_history


# In[54]:


theta = np.zeros(2)

# some gradient descent settings
iterations = 1500
alpha = 0.01

theta, J_history = gradientDescent(X ,y, theta, alpha, iterations)
print('Theta found by gradient descent: {:.4f}, {:.4f}'.format(*theta))
print('Expected theta values (approximately): [-3.6303, 1.1664]')


# In[58]:


plotData(X[:, 1], y)
pyplot.plot(X[:, 1], np.dot(X, theta), '-')
pyplot.legend(['Training data', 'Linear regression']);


# In[59]:


predict1 = np.dot([1, 3.5], theta)
print('For population = 35,000, we predict a profit of {:.2f}\n'.format(predict1*10000))

predict2 = np.dot([1, 7], theta)
print('For population = 70,000, we predict a profit of {:.2f}\n'.format(predict2*10000))


# In[60]:

# grid over which we will calculate J
theta0_vals = np.linspace(-10, 10, 100)
theta1_vals = np.linspace(-1, 4, 100)

# initialize J_vals to a matrix of 0's
J_vals = np.zeros((theta0_vals.shape[0], theta1_vals.shape[0]))

# Fill out J_vals
for i, theta0 in enumerate(theta0_vals):
    for j, theta1 in enumerate(theta1_vals):
        J_vals[i, j] = computeCost(X, y, [theta0, theta1])
        
# Because of the way meshgrids work in the surf command, we need to
# transpose J_vals before calling surf, or else the axes will be flipped
J_vals = J_vals.T

# surface plot
fig = pyplot.figure(figsize=(12, 5))
ax = fig.add_subplot(121, projection='3d')
ax.plot_surface(theta0_vals, theta1_vals, J_vals, cmap='viridis')
pyplot.xlabel('theta0')
pyplot.ylabel('theta1')
pyplot.title('Surface')

# contour plot
# Plot J_vals as 15 contours spaced logarithmically between 0.01 and 100
ax = pyplot.subplot(122)
pyplot.contour(theta0_vals, theta1_vals, J_vals, linewidths=2, cmap='viridis', levels=np.logspace(-2, 3, 20))
pyplot.xlabel('theta0')
pyplot.ylabel('theta1')
pyplot.plot(theta[0], theta[1], 'ro', ms=10, lw=2)
pyplot.title('Contour, showing minimum')
pass




